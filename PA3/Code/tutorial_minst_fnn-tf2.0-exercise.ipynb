{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, datasets\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n",
    "\n",
    "def mnist_dataset():\n",
    "    (x, y), (x_test, y_test) = datasets.mnist.load_data()\n",
    "    #normalize\n",
    "    x = x/255.0\n",
    "    x_test = x_test/255.0\n",
    "\n",
    "    # Flatten images from 28x28 to 784-dimensional vectors\n",
    "    x = x.reshape(-1, 28 * 28)  # Flatten to 784\n",
    "    x_test = x_test.reshape(-1, 28 * 28)  # Flatten to 784\n",
    "\n",
    "    return (x, y), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip([1, 2, 3, 4], ['a', 'b', 'c', 'd'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myModel:\n",
    "    def __init__(self):\n",
    "        ####################\n",
    "        '''声明模型对应的参数'''\n",
    "        ####################\n",
    "\n",
    "        # 第一层权重\n",
    "        self.W1 = tf.Variable(tf.random.normal([28*28, 128]), name=\"W1\")\n",
    "        # 第一层偏置\n",
    "        self.b1 = tf.Variable(tf.zeros([128]), name=\"b1\")\n",
    "        # 第二层权重\n",
    "        self.W2 = tf.Variable(tf.random.normal([128, 10]), name=\"W2\")\n",
    "        # 第二层偏置\n",
    "        self.b2 = tf.Variable(tf.zeros([10]), name=\"b2\")\n",
    "    def __call__(self, x):\n",
    "        ####################\n",
    "        '''实现模型函数体，返回未归一化的logits'''\n",
    "        ####################‘\n",
    "\n",
    "        # 线性变换 (x * W1 + b1)\n",
    "        h1 = tf.matmul(x, self.W1) + self.b1\n",
    "        # 使用 ReLU 激活函数\n",
    "        h1_relu = tf.nn.relu(h1)\n",
    "        # 第二层线性变换\n",
    "        logits = tf.matmul(h1_relu, self.W2) + self.b2\n",
    "        return logits\n",
    "\n",
    "model = myModel()\n",
    "\n",
    "optimizer = optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=labels))\n",
    "\n",
    "@tf.function\n",
    "def compute_accuracy(logits, labels):\n",
    "    predictions = tf.argmax(logits, axis=1)\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(predictions, labels), tf.float32))\n",
    "\n",
    "@tf.function\n",
    "def train_one_step(model, optimizer, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = compute_loss(logits, y)\n",
    "\n",
    "    # compute gradient\n",
    "    trainable_vars = [model.W1, model.W2, model.b1, model.b2]\n",
    "    grads = tape.gradient(loss, trainable_vars)\n",
    "    for g, v in zip(grads, trainable_vars):\n",
    "        v.assign_sub(0.01*g)\n",
    "\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "\n",
    "    # loss and accuracy is scalar tensor\n",
    "    return loss, accuracy\n",
    "\n",
    "@tf.function\n",
    "def test(model, x, y):\n",
    "    logits = model(x)\n",
    "    loss = compute_loss(logits, y)\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实际训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : loss 123.81343 ; accuracy 0.12038333\n",
      "epoch 1 : loss 106.462234 ; accuracy 0.12596667\n",
      "epoch 2 : loss 93.778534 ; accuracy 0.12836666\n",
      "epoch 3 : loss 84.7482 ; accuracy 0.12756667\n",
      "epoch 4 : loss 78.2506 ; accuracy 0.12605\n",
      "epoch 5 : loss 73.421555 ; accuracy 0.12516667\n",
      "epoch 6 : loss 69.70016 ; accuracy 0.12503333\n",
      "epoch 7 : loss 66.751526 ; accuracy 0.1266\n",
      "epoch 8 : loss 64.34164 ; accuracy 0.12831667\n",
      "epoch 9 : loss 62.310066 ; accuracy 0.13096666\n",
      "epoch 10 : loss 60.56888 ; accuracy 0.13413334\n",
      "epoch 11 : loss 59.047276 ; accuracy 0.13723333\n",
      "epoch 12 : loss 57.688374 ; accuracy 0.14078334\n",
      "epoch 13 : loss 56.453323 ; accuracy 0.1447\n",
      "epoch 14 : loss 55.314697 ; accuracy 0.1485\n",
      "epoch 15 : loss 54.2501 ; accuracy 0.15251666\n",
      "epoch 16 : loss 53.245125 ; accuracy 0.15741667\n",
      "epoch 17 : loss 52.289783 ; accuracy 0.1625\n",
      "epoch 18 : loss 51.375534 ; accuracy 0.16705\n",
      "epoch 19 : loss 50.49521 ; accuracy 0.17171666\n",
      "epoch 20 : loss 49.643974 ; accuracy 0.17618333\n",
      "epoch 21 : loss 48.81919 ; accuracy 0.18088333\n",
      "epoch 22 : loss 48.019176 ; accuracy 0.18558334\n",
      "epoch 23 : loss 47.242744 ; accuracy 0.19065\n",
      "epoch 24 : loss 46.48903 ; accuracy 0.1959\n",
      "epoch 25 : loss 45.757725 ; accuracy 0.20123333\n",
      "epoch 26 : loss 45.047855 ; accuracy 0.20598334\n",
      "epoch 27 : loss 44.357735 ; accuracy 0.21068333\n",
      "epoch 28 : loss 43.686657 ; accuracy 0.21568333\n",
      "epoch 29 : loss 43.034176 ; accuracy 0.22073333\n",
      "epoch 30 : loss 42.399773 ; accuracy 0.22558333\n",
      "epoch 31 : loss 41.783104 ; accuracy 0.23075\n",
      "epoch 32 : loss 41.18376 ; accuracy 0.236\n",
      "epoch 33 : loss 40.60106 ; accuracy 0.24133334\n",
      "epoch 34 : loss 40.03407 ; accuracy 0.24628334\n",
      "epoch 35 : loss 39.482292 ; accuracy 0.2513\n",
      "epoch 36 : loss 38.945232 ; accuracy 0.25598332\n",
      "epoch 37 : loss 38.422276 ; accuracy 0.26055\n",
      "epoch 38 : loss 37.912827 ; accuracy 0.26528335\n",
      "epoch 39 : loss 37.41623 ; accuracy 0.26956666\n",
      "epoch 40 : loss 36.931843 ; accuracy 0.27408335\n",
      "epoch 41 : loss 36.459534 ; accuracy 0.27903333\n",
      "epoch 42 : loss 35.999134 ; accuracy 0.28375\n",
      "epoch 43 : loss 35.550312 ; accuracy 0.28815\n",
      "epoch 44 : loss 35.11265 ; accuracy 0.29315\n",
      "epoch 45 : loss 34.685658 ; accuracy 0.29746667\n",
      "epoch 46 : loss 34.268932 ; accuracy 0.30188334\n",
      "epoch 47 : loss 33.862053 ; accuracy 0.30596668\n",
      "epoch 48 : loss 33.46463 ; accuracy 0.30991668\n",
      "epoch 49 : loss 33.07641 ; accuracy 0.31423333\n",
      "epoch 50 : loss 32.696976 ; accuracy 0.3181\n",
      "epoch 51 : loss 32.326065 ; accuracy 0.32208332\n",
      "epoch 52 : loss 31.963547 ; accuracy 0.32595\n",
      "epoch 53 : loss 31.609085 ; accuracy 0.33\n",
      "epoch 54 : loss 31.262444 ; accuracy 0.33398333\n",
      "epoch 55 : loss 30.923313 ; accuracy 0.3377\n",
      "epoch 56 : loss 30.591507 ; accuracy 0.34151667\n",
      "epoch 57 : loss 30.266884 ; accuracy 0.34526667\n",
      "epoch 58 : loss 29.949255 ; accuracy 0.34896666\n",
      "epoch 59 : loss 29.638458 ; accuracy 0.35258332\n",
      "epoch 60 : loss 29.334284 ; accuracy 0.35591668\n",
      "epoch 61 : loss 29.036512 ; accuracy 0.35963333\n",
      "epoch 62 : loss 28.74502 ; accuracy 0.36323333\n",
      "epoch 63 : loss 28.459625 ; accuracy 0.36698332\n",
      "epoch 64 : loss 28.180183 ; accuracy 0.3701\n",
      "epoch 65 : loss 27.906534 ; accuracy 0.37346667\n",
      "epoch 66 : loss 27.638487 ; accuracy 0.37681666\n",
      "epoch 67 : loss 27.375889 ; accuracy 0.38016668\n",
      "epoch 68 : loss 27.118591 ; accuracy 0.38356668\n",
      "epoch 69 : loss 26.866514 ; accuracy 0.38716668\n",
      "epoch 70 : loss 26.619453 ; accuracy 0.3901\n",
      "epoch 71 : loss 26.377258 ; accuracy 0.39343333\n",
      "epoch 72 : loss 26.139788 ; accuracy 0.39693335\n",
      "epoch 73 : loss 25.906948 ; accuracy 0.40031666\n",
      "epoch 74 : loss 25.678646 ; accuracy 0.4034\n",
      "epoch 75 : loss 25.454683 ; accuracy 0.40651667\n",
      "epoch 76 : loss 25.234934 ; accuracy 0.40971667\n",
      "epoch 77 : loss 25.019207 ; accuracy 0.41231668\n",
      "epoch 78 : loss 24.80738 ; accuracy 0.41551667\n",
      "epoch 79 : loss 24.59936 ; accuracy 0.41845\n",
      "epoch 80 : loss 24.394997 ; accuracy 0.42108333\n",
      "epoch 81 : loss 24.194214 ; accuracy 0.42345\n",
      "epoch 82 : loss 23.996958 ; accuracy 0.42656666\n",
      "epoch 83 : loss 23.803146 ; accuracy 0.42908335\n",
      "epoch 84 : loss 23.612741 ; accuracy 0.43195\n",
      "epoch 85 : loss 23.4257 ; accuracy 0.43446666\n",
      "epoch 86 : loss 23.241938 ; accuracy 0.43716666\n",
      "epoch 87 : loss 23.061396 ; accuracy 0.43963334\n",
      "epoch 88 : loss 22.884008 ; accuracy 0.4427\n",
      "epoch 89 : loss 22.709694 ; accuracy 0.44523335\n",
      "epoch 90 : loss 22.538357 ; accuracy 0.44791666\n",
      "epoch 91 : loss 22.369932 ; accuracy 0.45025\n",
      "epoch 92 : loss 22.20436 ; accuracy 0.45255\n",
      "epoch 93 : loss 22.041538 ; accuracy 0.4547\n",
      "epoch 94 : loss 21.881409 ; accuracy 0.45721668\n",
      "epoch 95 : loss 21.723925 ; accuracy 0.45995\n",
      "epoch 96 : loss 21.569012 ; accuracy 0.46221668\n",
      "epoch 97 : loss 21.416567 ; accuracy 0.46456668\n",
      "epoch 98 : loss 21.266548 ; accuracy 0.4666\n",
      "epoch 99 : loss 21.1189 ; accuracy 0.46885\n",
      "epoch 100 : loss 20.973515 ; accuracy 0.47101668\n",
      "epoch 101 : loss 20.830357 ; accuracy 0.47325\n",
      "epoch 102 : loss 20.689365 ; accuracy 0.47566667\n",
      "epoch 103 : loss 20.550533 ; accuracy 0.47781667\n",
      "epoch 104 : loss 20.413818 ; accuracy 0.47995\n",
      "epoch 105 : loss 20.279177 ; accuracy 0.4818\n",
      "epoch 106 : loss 20.146566 ; accuracy 0.48406667\n",
      "epoch 107 : loss 20.01597 ; accuracy 0.48611668\n",
      "epoch 108 : loss 19.887327 ; accuracy 0.4882\n",
      "epoch 109 : loss 19.760592 ; accuracy 0.4904\n",
      "epoch 110 : loss 19.635738 ; accuracy 0.4923\n",
      "epoch 111 : loss 19.512714 ; accuracy 0.4944\n",
      "epoch 112 : loss 19.391489 ; accuracy 0.49615\n",
      "epoch 113 : loss 19.27204 ; accuracy 0.49818334\n",
      "epoch 114 : loss 19.15433 ; accuracy 0.49995\n",
      "epoch 115 : loss 19.03831 ; accuracy 0.50185\n",
      "epoch 116 : loss 18.92395 ; accuracy 0.50395\n",
      "epoch 117 : loss 18.811226 ; accuracy 0.5058\n",
      "epoch 118 : loss 18.700115 ; accuracy 0.5074667\n",
      "epoch 119 : loss 18.590565 ; accuracy 0.50913334\n",
      "epoch 120 : loss 18.482563 ; accuracy 0.51085\n",
      "epoch 121 : loss 18.376076 ; accuracy 0.51295\n",
      "epoch 122 : loss 18.27112 ; accuracy 0.51465\n",
      "epoch 123 : loss 18.16767 ; accuracy 0.51641667\n",
      "epoch 124 : loss 18.065681 ; accuracy 0.51816666\n",
      "epoch 125 : loss 17.96513 ; accuracy 0.5197833\n",
      "epoch 126 : loss 17.865974 ; accuracy 0.5215167\n",
      "epoch 127 : loss 17.768187 ; accuracy 0.52355\n",
      "epoch 128 : loss 17.671713 ; accuracy 0.52501667\n",
      "epoch 129 : loss 17.576519 ; accuracy 0.5269333\n",
      "epoch 130 : loss 17.482582 ; accuracy 0.52881664\n",
      "epoch 131 : loss 17.389889 ; accuracy 0.5306\n",
      "epoch 132 : loss 17.298424 ; accuracy 0.53208333\n",
      "epoch 133 : loss 17.208178 ; accuracy 0.53365\n",
      "epoch 134 : loss 17.119143 ; accuracy 0.5352\n",
      "epoch 135 : loss 17.031271 ; accuracy 0.5368\n",
      "epoch 136 : loss 16.944551 ; accuracy 0.53858334\n",
      "epoch 137 : loss 16.859001 ; accuracy 0.54011667\n",
      "epoch 138 : loss 16.774597 ; accuracy 0.54191667\n",
      "epoch 139 : loss 16.691278 ; accuracy 0.54358333\n",
      "epoch 140 : loss 16.60904 ; accuracy 0.5455\n",
      "epoch 141 : loss 16.527857 ; accuracy 0.54711664\n",
      "epoch 142 : loss 16.447704 ; accuracy 0.5485167\n",
      "epoch 143 : loss 16.368555 ; accuracy 0.54983336\n",
      "epoch 144 : loss 16.290379 ; accuracy 0.5512\n",
      "epoch 145 : loss 16.21318 ; accuracy 0.5525\n",
      "epoch 146 : loss 16.136913 ; accuracy 0.5539\n",
      "epoch 147 : loss 16.06156 ; accuracy 0.5553167\n",
      "epoch 148 : loss 15.987108 ; accuracy 0.55668336\n",
      "epoch 149 : loss 15.913529 ; accuracy 0.5578667\n",
      "epoch 150 : loss 15.840808 ; accuracy 0.55906665\n",
      "epoch 151 : loss 15.768942 ; accuracy 0.56053334\n",
      "epoch 152 : loss 15.697919 ; accuracy 0.5618333\n",
      "epoch 153 : loss 15.627723 ; accuracy 0.5632167\n",
      "epoch 154 : loss 15.5583315 ; accuracy 0.56476665\n",
      "epoch 155 : loss 15.489729 ; accuracy 0.56605\n",
      "epoch 156 : loss 15.42191 ; accuracy 0.56725\n",
      "epoch 157 : loss 15.354868 ; accuracy 0.56848335\n",
      "epoch 158 : loss 15.288568 ; accuracy 0.5699\n",
      "epoch 159 : loss 15.223013 ; accuracy 0.57123333\n",
      "epoch 160 : loss 15.158223 ; accuracy 0.5726167\n",
      "epoch 161 : loss 15.094165 ; accuracy 0.57355\n",
      "epoch 162 : loss 15.030826 ; accuracy 0.57496667\n",
      "epoch 163 : loss 14.968196 ; accuracy 0.57615\n",
      "epoch 164 : loss 14.906258 ; accuracy 0.5775\n",
      "epoch 165 : loss 14.844998 ; accuracy 0.5786167\n",
      "epoch 166 : loss 14.784414 ; accuracy 0.57956666\n",
      "epoch 167 : loss 14.724492 ; accuracy 0.58068335\n",
      "epoch 168 : loss 14.665216 ; accuracy 0.58215\n",
      "epoch 169 : loss 14.606573 ; accuracy 0.58316666\n",
      "epoch 170 : loss 14.548541 ; accuracy 0.58426666\n",
      "epoch 171 : loss 14.491115 ; accuracy 0.5854833\n",
      "epoch 172 : loss 14.434291 ; accuracy 0.58671665\n",
      "epoch 173 : loss 14.378069 ; accuracy 0.5879\n",
      "epoch 174 : loss 14.322429 ; accuracy 0.5891167\n",
      "epoch 175 : loss 14.267367 ; accuracy 0.5902\n",
      "epoch 176 : loss 14.212871 ; accuracy 0.5913333\n",
      "epoch 177 : loss 14.158929 ; accuracy 0.5926\n",
      "epoch 178 : loss 14.105528 ; accuracy 0.5935\n",
      "epoch 179 : loss 14.052662 ; accuracy 0.59445\n",
      "epoch 180 : loss 14.00032 ; accuracy 0.59548336\n",
      "epoch 181 : loss 13.948504 ; accuracy 0.59638333\n",
      "epoch 182 : loss 13.897198 ; accuracy 0.5973667\n",
      "epoch 183 : loss 13.8463955 ; accuracy 0.59828335\n",
      "epoch 184 : loss 13.796094 ; accuracy 0.59936666\n",
      "epoch 185 : loss 13.746292 ; accuracy 0.6005333\n",
      "epoch 186 : loss 13.696975 ; accuracy 0.60143334\n",
      "epoch 187 : loss 13.648153 ; accuracy 0.60251665\n",
      "epoch 188 : loss 13.599817 ; accuracy 0.6034667\n",
      "epoch 189 : loss 13.551953 ; accuracy 0.6044667\n",
      "epoch 190 : loss 13.504559 ; accuracy 0.60543334\n",
      "epoch 191 : loss 13.457633 ; accuracy 0.60656667\n",
      "epoch 192 : loss 13.411173 ; accuracy 0.6075\n",
      "epoch 193 : loss 13.365165 ; accuracy 0.60855\n",
      "epoch 194 : loss 13.319602 ; accuracy 0.60941666\n",
      "epoch 195 : loss 13.274481 ; accuracy 0.61043334\n",
      "epoch 196 : loss 13.2298 ; accuracy 0.6114333\n",
      "epoch 197 : loss 13.185548 ; accuracy 0.6125\n",
      "epoch 198 : loss 13.141707 ; accuracy 0.61343336\n",
      "epoch 199 : loss 13.098272 ; accuracy 0.61445\n",
      "test loss 12.801384 ; accuracy 0.6178\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = mnist_dataset()\n",
    "for epoch in range(200):\n",
    "    loss, accuracy = train_one_step(model, optimizer,\n",
    "                                    tf.constant(train_data[0], dtype=tf.float32),\n",
    "                                    tf.constant(train_data[1], dtype=tf.int64))\n",
    "    print('epoch', epoch, ': loss', loss.numpy(), '; accuracy', accuracy.numpy())\n",
    "loss, accuracy = test(model,\n",
    "                      tf.constant(test_data[0], dtype=tf.float32),\n",
    "                      tf.constant(test_data[1], dtype=tf.int64))\n",
    "\n",
    "print('test loss', loss.numpy(), '; accuracy', accuracy.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
